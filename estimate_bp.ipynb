{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline loaded for Bryan:\n",
      "  HR: 86.2 bpm\n",
      "  mNPV: 0.012308\n",
      "  BP: 120/62 mmHg\n",
      "  MAP: 81.3 mmHg\n",
      "\n",
      "Processing 13 training files...\n",
      "  Processed bryan_baseline.csv (1/13)\n",
      "  Processed 30_apr_bryan_6.csv (2/13)\n",
      "  Processed 30_apr_bryan_7.csv (3/13)\n",
      "  Processed 30_apr_bryan_5.csv (4/13)\n",
      "  Processed 30_apr_bryan_4.csv (5/13)\n",
      "  Processed 30_apr_bryan_1.csv (6/13)\n",
      "  Processed 30_apr_bryan_3.csv (7/13)\n",
      "  Skipping 30_apr_bryan_2.csv: No ground truth data\n",
      "  Skipping test8_bryan_rest.csv: No ground truth data\n",
      "  Processed 23apr_bryan_5.csv (10/13)\n",
      "  Processed 23apr_bryan_4.csv (11/13)\n",
      "  Processed 23apr_bryan_3.csv (12/13)\n",
      "  Processed 23apr_bryan_2.csv (13/13)\n",
      "\n",
      "Training models with 11 data points...\n",
      "\n",
      "Model training complete:\n",
      "  SBP model R²: 0.320\n",
      "  DBP model R²: 0.373\n",
      "  MAP model R²: 0.137\n",
      "\n",
      "Processing 13 training files...\n",
      "  Processed bryan_baseline.csv (1/13)\n",
      "  Processed 30_apr_bryan_6.csv (2/13)\n",
      "  Processed 30_apr_bryan_7.csv (3/13)\n",
      "  Processed 30_apr_bryan_5.csv (4/13)\n",
      "  Processed 30_apr_bryan_4.csv (5/13)\n",
      "  Processed 30_apr_bryan_1.csv (6/13)\n",
      "  Processed 30_apr_bryan_3.csv (7/13)\n",
      "  Skipping 30_apr_bryan_2.csv: No ground truth data\n",
      "  Skipping test8_bryan_rest.csv: No ground truth data\n",
      "  Processed 23apr_bryan_5.csv (10/13)\n",
      "  Processed 23apr_bryan_4.csv (11/13)\n",
      "  Processed 23apr_bryan_3.csv (12/13)\n",
      "  Processed 23apr_bryan_2.csv (13/13)\n",
      "\n",
      "Training models with 11 data points...\n",
      "\n",
      "Model training complete:\n",
      "  SBP model R²: 0.320\n",
      "  DBP model R²: 0.373\n",
      "  MAP model R²: 0.137\n",
      "\n",
      "Model saved to /Users/bryanjangeesingh/Documents/6.1820/KardIoT/data/Bryan/Bryan_model.pkl\n",
      "JSON copy saved to /Users/bryanjangeesingh/Documents/6.1820/KardIoT/data/Bryan/Bryan_model.json\n",
      "\n",
      "Model ready for inference!\n",
      "\n",
      "Example prediction for HR=80.0, mNPV=0.0150:\n",
      "  Estimated BP: 124.8/65.1 mmHg, MAP: 85.1 mmHg\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "PPG-based Blood Pressure Model Training Script\n",
    "Usage: python train_ppg_model.py --person <name> --dir <path>\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PersonModel:\n",
    "    \"\"\"Data class to store person-specific model parameters\"\"\"\n",
    "\n",
    "    name: str\n",
    "    baseline_hr: float\n",
    "    baseline_mnpv: float\n",
    "    baseline_sbp: float\n",
    "    baseline_dbp: float\n",
    "    baseline_map: float\n",
    "    sbp_coeffs: Tuple[float, float, float]  # (a, b, c)\n",
    "    dbp_coeffs: Tuple[float, float, float]\n",
    "    map_coeffs: Tuple[float, float, float]\n",
    "    r2_scores: Dict[str, float]  # R² scores for each model\n",
    "\n",
    "\n",
    "class PPGBloodPressureModel:\n",
    "    def __init__(self, person_name: str, data_dir: str):\n",
    "        self.person_name = person_name\n",
    "        self.data_dir = data_dir\n",
    "        self.baseline_file = os.path.join(data_dir, f\"{person_name}_baseline.csv\")\n",
    "        self.model_file = os.path.join(data_dir, f\"{person_name}_model.pkl\")\n",
    "\n",
    "        # Model parameters\n",
    "        self.baseline_hr = None\n",
    "        self.baseline_mnpv = None\n",
    "        self.baseline_sbp = None\n",
    "        self.baseline_dbp = None\n",
    "        self.baseline_map = None\n",
    "\n",
    "        # Regression models\n",
    "        self.sbp_model = LinearRegression()\n",
    "        self.dbp_model = LinearRegression()\n",
    "        self.map_model = LinearRegression()\n",
    "\n",
    "    def load_ppg_data(self, file_path: str) -> np.ndarray:\n",
    "        \"\"\"Load PPG data from CSV file\"\"\"\n",
    "        time_vals = []\n",
    "        signal_vals = []\n",
    "        ground_truth = None\n",
    "\n",
    "        with open(file_path, \"r\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                parts = line.strip().split(\",\")\n",
    "                if len(parts) < 2:\n",
    "                    continue\n",
    "\n",
    "                # First line contains ground truth values\n",
    "                if i == 0 and len(parts) >= 5:\n",
    "                    try:\n",
    "                        _, _, sbp, dbp, hr = parts[:5]\n",
    "                        ground_truth = {\n",
    "                            \"sbp\": float(sbp),\n",
    "                            \"dbp\": float(dbp),\n",
    "                            \"hr\": float(hr),\n",
    "                        }\n",
    "                    except (ValueError, IndexError):\n",
    "                        pass\n",
    "\n",
    "                try:\n",
    "                    t = float(parts[0])\n",
    "                    s = float(parts[1])\n",
    "                    time_vals.append(t * 1e-3)  # convert ms to s\n",
    "                    signal_vals.append(s)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "        data = np.column_stack((time_vals, signal_vals))\n",
    "        return data, ground_truth\n",
    "\n",
    "    def calculate_ppg_features(self, time: np.ndarray, signal: np.ndarray) -> Dict:\n",
    "        \"\"\"Calculate PPG features from time and signal arrays\"\"\"\n",
    "        # Sampling frequency\n",
    "        fs = 1.0 / np.mean(np.diff(time))\n",
    "\n",
    "        # DC level\n",
    "        dc = np.mean(signal)\n",
    "\n",
    "        # Find peaks and troughs\n",
    "        peaks, _ = find_peaks(signal, prominence=4, distance=fs * 0.3)\n",
    "        troughs, _ = find_peaks(-signal, prominence=4, distance=fs * 0.3)\n",
    "\n",
    "        # AC amplitude\n",
    "        n = min(len(peaks), len(troughs))\n",
    "        if n == 0:\n",
    "            ac = np.nan\n",
    "        else:\n",
    "            ac_vals = signal[peaks[:n]] - signal[troughs[:n]]\n",
    "            ac = np.mean(np.abs(ac_vals))\n",
    "\n",
    "        # Heart rate\n",
    "        if len(peaks) > 1:\n",
    "            intervals = np.diff(time[peaks])\n",
    "            hr = 60.0 / np.mean(intervals)\n",
    "        else:\n",
    "            hr = np.nan\n",
    "\n",
    "        # mNPV\n",
    "        mnpv = ac / dc if dc != 0 else np.nan\n",
    "\n",
    "        return {\n",
    "            \"hr\": hr,\n",
    "            \"ac\": ac,\n",
    "            \"dc\": dc,\n",
    "            \"mnpv\": mnpv,\n",
    "            \"peaks\": peaks,\n",
    "            \"troughs\": troughs,\n",
    "        }\n",
    "\n",
    "    def load_baseline(self) -> None:\n",
    "        \"\"\"Load baseline values from baseline CSV file\"\"\"\n",
    "        if not os.path.exists(self.baseline_file):\n",
    "            raise FileNotFoundError(f\"Baseline file not found: {self.baseline_file}\")\n",
    "\n",
    "        ppg_data, ground_truth = self.load_ppg_data(self.baseline_file)\n",
    "        time, signal = ppg_data[:, 0], ppg_data[:, 1]\n",
    "\n",
    "        features = self.calculate_ppg_features(time, signal)\n",
    "\n",
    "        self.baseline_hr = features[\"hr\"]\n",
    "        self.baseline_mnpv = features[\"mnpv\"]\n",
    "        self.baseline_sbp = ground_truth[\"sbp\"]\n",
    "        self.baseline_dbp = ground_truth[\"dbp\"]\n",
    "        self.baseline_map = self.baseline_dbp + (1 / 3) * (\n",
    "            self.baseline_sbp - self.baseline_dbp\n",
    "        )\n",
    "\n",
    "        print(f\"Baseline loaded for {self.person_name}:\")\n",
    "        print(f\"  HR: {self.baseline_hr:.1f} bpm\")\n",
    "        print(f\"  mNPV: {self.baseline_mnpv:.6f}\")\n",
    "        print(f\"  BP: {self.baseline_sbp:.0f}/{self.baseline_dbp:.0f} mmHg\")\n",
    "        print(f\"  MAP: {self.baseline_map:.1f} mmHg\")\n",
    "\n",
    "    def calculate_deltas(\n",
    "        self,\n",
    "        current_hr: float,\n",
    "        current_mnpv: float,\n",
    "        current_sbp: float,\n",
    "        current_dbp: float,\n",
    "    ) -> Tuple[float, ...]:\n",
    "        \"\"\"Calculate delta values from baseline\"\"\"\n",
    "        current_map = current_dbp + (1 / 3) * (current_sbp - current_dbp)\n",
    "\n",
    "        delta_ln_hr = np.log(current_hr) - np.log(self.baseline_hr)\n",
    "        delta_ln_mnpv = np.log(current_mnpv) - np.log(self.baseline_mnpv)\n",
    "        delta_ln_sbp = np.log(current_sbp) - np.log(self.baseline_sbp)\n",
    "        delta_ln_dbp = np.log(current_dbp) - np.log(self.baseline_dbp)\n",
    "        delta_ln_map = np.log(current_map) - np.log(self.baseline_map)\n",
    "\n",
    "        return delta_ln_hr, delta_ln_mnpv, delta_ln_sbp, delta_ln_dbp, delta_ln_map\n",
    "\n",
    "    def process_training_data(self) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:\n",
    "        \"\"\"Process all training CSV files in the directory\"\"\"\n",
    "        delta_data = {\"hr\": [], \"mnpv\": [], \"sbp\": [], \"dbp\": [], \"map\": []}\n",
    "\n",
    "        # Get all CSV files except baseline\n",
    "        csv_files = [\n",
    "            f\n",
    "            for f in os.listdir(self.data_dir)\n",
    "            if f.endswith(\".csv\") and f != f\"{self.person_name}_baseline.csv\"\n",
    "        ]\n",
    "\n",
    "        print(f\"\\nProcessing {len(csv_files)} training files...\")\n",
    "\n",
    "        for i, csv_file in enumerate(csv_files):\n",
    "            file_path = os.path.join(self.data_dir, csv_file)\n",
    "            ppg_data, ground_truth = self.load_ppg_data(file_path)\n",
    "\n",
    "            if ground_truth is None:\n",
    "                print(f\"  Skipping {csv_file}: No ground truth data\")\n",
    "                continue\n",
    "\n",
    "            time, signal = ppg_data[:, 0], ppg_data[:, 1]\n",
    "            features = self.calculate_ppg_features(time, signal)\n",
    "\n",
    "            if np.isnan(features[\"hr\"]) or np.isnan(features[\"mnpv\"]):\n",
    "                print(f\"  Skipping {csv_file}: Invalid features\")\n",
    "                continue\n",
    "\n",
    "            # Calculate deltas\n",
    "            deltas = self.calculate_deltas(\n",
    "                features[\"hr\"],\n",
    "                features[\"mnpv\"],\n",
    "                ground_truth[\"sbp\"],\n",
    "                ground_truth[\"dbp\"],\n",
    "            )\n",
    "\n",
    "            delta_data[\"hr\"].append(deltas[0])\n",
    "            delta_data[\"mnpv\"].append(deltas[1])\n",
    "            delta_data[\"sbp\"].append(deltas[2])\n",
    "            delta_data[\"dbp\"].append(deltas[3])\n",
    "            delta_data[\"map\"].append(deltas[4])\n",
    "\n",
    "            print(f\"  Processed {csv_file} ({i+1}/{len(csv_files)})\")\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        for key in delta_data:\n",
    "            delta_data[key] = np.array(delta_data[key])\n",
    "\n",
    "        X = np.column_stack((delta_data[\"hr\"], delta_data[\"mnpv\"]))\n",
    "\n",
    "        return X, delta_data\n",
    "\n",
    "    def train_models(self) -> None:\n",
    "        \"\"\"Train regression models on the processed data\"\"\"\n",
    "        X, delta_data = self.process_training_data()\n",
    "\n",
    "        if len(X) == 0:\n",
    "            raise ValueError(\"No valid training data found\")\n",
    "\n",
    "        print(f\"\\nTraining models with {len(X)} data points...\")\n",
    "\n",
    "        # Train SBP model\n",
    "        self.sbp_model.fit(X, delta_data[\"sbp\"])\n",
    "        sbp_r2 = self.sbp_model.score(X, delta_data[\"sbp\"])\n",
    "\n",
    "        # Train DBP model\n",
    "        self.dbp_model.fit(X, delta_data[\"dbp\"])\n",
    "        dbp_r2 = self.dbp_model.score(X, delta_data[\"dbp\"])\n",
    "\n",
    "        # Train MAP model\n",
    "        self.map_model.fit(X, delta_data[\"map\"])\n",
    "        map_r2 = self.map_model.score(X, delta_data[\"map\"])\n",
    "\n",
    "        print(\"\\nModel training complete:\")\n",
    "        print(f\"  SBP model R²: {sbp_r2:.3f}\")\n",
    "        print(f\"  DBP model R²: {dbp_r2:.3f}\")\n",
    "        print(f\"  MAP model R²: {map_r2:.3f}\")\n",
    "\n",
    "        return {\"sbp\": sbp_r2, \"dbp\": dbp_r2, \"map\": map_r2}\n",
    "\n",
    "    def save_model(self) -> None:\n",
    "        \"\"\"Save trained model to file\"\"\"\n",
    "        model_data = PersonModel(\n",
    "            name=self.person_name,\n",
    "            baseline_hr=self.baseline_hr,\n",
    "            baseline_mnpv=self.baseline_mnpv,\n",
    "            baseline_sbp=self.baseline_sbp,\n",
    "            baseline_dbp=self.baseline_dbp,\n",
    "            baseline_map=self.baseline_map,\n",
    "            sbp_coeffs=(\n",
    "                self.sbp_model.coef_[0],\n",
    "                self.sbp_model.coef_[1],\n",
    "                self.sbp_model.intercept_,\n",
    "            ),\n",
    "            dbp_coeffs=(\n",
    "                self.dbp_model.coef_[0],\n",
    "                self.dbp_model.coef_[1],\n",
    "                self.dbp_model.intercept_,\n",
    "            ),\n",
    "            map_coeffs=(\n",
    "                self.map_model.coef_[0],\n",
    "                self.map_model.coef_[1],\n",
    "                self.map_model.intercept_,\n",
    "            ),\n",
    "            r2_scores=self.train_models(),\n",
    "        )\n",
    "\n",
    "        with open(self.model_file, \"wb\") as f:\n",
    "            pickle.dump(model_data, f)\n",
    "\n",
    "        # Also save as JSON for readability\n",
    "        json_file = self.model_file.replace(\".pkl\", \".json\")\n",
    "        with open(json_file, \"w\") as f:\n",
    "            json.dump(asdict(model_data), f, indent=2)\n",
    "\n",
    "        print(f\"\\nModel saved to {self.model_file}\")\n",
    "        print(f\"JSON copy saved to {json_file}\")\n",
    "\n",
    "    def load_model(self) -> PersonModel:\n",
    "        \"\"\"Load trained model from file\"\"\"\n",
    "        if not os.path.exists(self.model_file):\n",
    "            raise FileNotFoundError(f\"Model file not found: {self.model_file}\")\n",
    "\n",
    "        with open(self.model_file, \"rb\") as f:\n",
    "            model_data = pickle.load(f)\n",
    "\n",
    "        # Update instance variables\n",
    "        self.baseline_hr = model_data.baseline_hr\n",
    "        self.baseline_mnpv = model_data.baseline_mnpv\n",
    "        self.baseline_sbp = model_data.baseline_sbp\n",
    "        self.baseline_dbp = model_data.baseline_dbp\n",
    "        self.baseline_map = model_data.baseline_map\n",
    "\n",
    "        # Update regression models\n",
    "        self.sbp_model.coef_ = np.array(model_data.sbp_coeffs[:2])\n",
    "        self.sbp_model.intercept_ = model_data.sbp_coeffs[2]\n",
    "\n",
    "        self.dbp_model.coef_ = np.array(model_data.dbp_coeffs[:2])\n",
    "        self.dbp_model.intercept_ = model_data.dbp_coeffs[2]\n",
    "\n",
    "        self.map_model.coef_ = np.array(model_data.map_coeffs[:2])\n",
    "        self.map_model.intercept_ = model_data.map_coeffs[2]\n",
    "\n",
    "        print(f\"Model loaded for {self.person_name}\")\n",
    "        return model_data\n",
    "\n",
    "    def estimate_blood_pressure(\n",
    "        self, hr: float, mnpv: float\n",
    "    ) -> Tuple[float, float, float]:\n",
    "        \"\"\"Estimate blood pressure from HR and mNPV values\"\"\"\n",
    "        delta_ln_hr = np.log(hr) - np.log(self.baseline_hr)\n",
    "        delta_ln_mnpv = np.log(mnpv) - np.log(self.baseline_mnpv)\n",
    "\n",
    "        X = np.array([[delta_ln_hr, delta_ln_mnpv]])\n",
    "\n",
    "        est_delta_ln_sbp = self.sbp_model.predict(X)[0]\n",
    "        est_delta_ln_dbp = self.dbp_model.predict(X)[0]\n",
    "        est_delta_ln_map = self.map_model.predict(X)[0]\n",
    "\n",
    "        est_sbp = self.baseline_sbp * np.exp(est_delta_ln_sbp)\n",
    "        est_dbp = self.baseline_dbp * np.exp(est_delta_ln_dbp)\n",
    "        est_map = self.baseline_map * np.exp(est_delta_ln_map)\n",
    "\n",
    "        return est_sbp, est_dbp, est_map\n",
    "\n",
    "\n",
    "def main(person_name, person_folder):\n",
    "    model = PPGBloodPressureModel(person_name, person_folder)\n",
    "\n",
    "    model.load_baseline()\n",
    "    model.train_models()\n",
    "    model.save_model()\n",
    "\n",
    "    print(\"\\nModel ready for inference!\")\n",
    "\n",
    "    # Example usage\n",
    "    example_hr = 80\n",
    "    example_mnpv = 0.015\n",
    "    est_sbp, est_dbp, est_map = model.estimate_blood_pressure(example_hr, example_mnpv)\n",
    "    print(f\"\\nExample prediction for HR={example_hr:.1f}, mNPV={example_mnpv:.4f}:\")\n",
    "    print(f\"  Estimated BP: {est_sbp:.1f}/{est_dbp:.1f} mmHg, MAP: {est_map:.1f} mmHg\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"Bryan\", \"/Users/bryanjangeesingh/Documents/6.1820/KardIoT/data/Bryan\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
